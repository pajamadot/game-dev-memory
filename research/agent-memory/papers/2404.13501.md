# A Survey on the Memory Mechanism of Large Language Model based Agents

- arXiv: 2404.13501
- URL: https://arxiv.org/abs/2404.13501v1
- PDF: https://arxiv.org/pdf/2404.13501v1
- Published: 2024-04-21T01:49:46Z
- Updated: 2024-04-21T01:49:46Z
- Categories: cs.AI
- Authors: Zeyu Zhang, Xiaohe Bo, Chen Ma, Rui Li, Xu Chen, Quanyu Dai, Jieming Zhu, Zhenhua Dong, Ji-Rong Wen

## Abstract

Large language model (LLM) based agents have recently attracted much attention from the research and industry communities. Compared with original LLMs, LLM-based agents are featured in their self-evolving capability, which is the basis for solving real-world problems that need long-term and complex agent-environment interactions. The key component to support agent-environment interactions is the memory of the agents. While previous studies have proposed many promising memory mechanisms, they are scattered in different papers, and there lacks a systematical review to summarize and compare these works from a holistic perspective, failing to abstract common and effective designing patterns for inspiring future studies. To bridge this gap, in this paper, we propose a comprehensive survey on the memory mechanism of LLM-based agents. In specific, we first discuss ''what is'' and ''why do we need'' the memory in LLM-based agents. Then, we systematically review previous studies on how to design and evaluate the memory module. In addition, we also present many agent applications, where the memory module plays an important role. At last, we analyze the limitations of existing work and show important future directions. To keep up with the latest advances in this field, we create a repository at \url{https://github.com/nuster1128/LLM_Agent_Memory_Survey}.

## What This Adds (fill in)

- Problem: Memory is a key enabler for "self-evolving" agents that must operate across long horizons and sessions.
- Memory mechanism (write/update/retrieve/forget): Provides a taxonomy and survey of memory module design + evaluation across many agent papers; highlights patterns for acquisition, storage, retrieval, and updating/forgetting.
- Evaluation / benchmarks: Calls out the lack of unified evaluation and surveys existing evaluation approaches across applications.
- Failure modes / tradeoffs: The field is fragmented; designs are often app-specific; without strong update policies, agents can amplify outdated or wrong memories.

## How This Maps To game-dev-memory (fill in)

- What to store (tables/assets): Keep distinct memory categories (episodic/semantic/procedural) and separate evidence assets from derived summaries/beliefs.
- Retrieval pattern: Hard filters by tenant/project/session + category; progressive retrieval (evidence first) before synthesis.
- Evolution trigger: Per-session consolidation (summaries, dedupe, confidence adjustment) plus periodic "maintenance" jobs (prune/supersede).
- UI affordance: Make memory types visible in UI; show provenance and "why retrieved" to reduce silent failure.
