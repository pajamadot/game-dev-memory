# LongMemEval: Benchmarking Chat Assistants on Long-Term Interactive Memory

- arXiv: 2410.10813
- URL: https://arxiv.org/abs/2410.10813v2
- PDF: https://arxiv.org/pdf/2410.10813v2
- Published: 2024-10-14T17:59:44Z
- Updated: 2025-03-04T22:19:41Z
- Categories: cs.CL
- Authors: Di Wu, Hongwei Wang, Wenhao Yu, Yuwei Zhang, Kai-Wei Chang, Dong Yu

## Abstract

Recent large language model (LLM)-driven chat assistant systems have integrated memory components to track user-assistant chat histories, enabling more accurate and personalized responses. However, their long-term memory capabilities in sustained interactions remain underexplored. We introduce LongMemEval, a comprehensive benchmark designed to evaluate five core long-term memory abilities of chat assistants: information extraction, multi-session reasoning, temporal reasoning, knowledge updates, and abstention. With 500 meticulously curated questions embedded within freely scalable user-assistant chat histories, LongMemEval presents a significant challenge to existing long-term memory systems, with commercial chat assistants and long-context LLMs showing a 30% accuracy drop on memorizing information across sustained interactions. We then present a unified framework that breaks down the long-term memory design into three stages: indexing, retrieval, and reading. Built upon key experimental insights, we propose several memory design optimizations including session decomposition for value granularity, fact-augmented key expansion for indexing, and time-aware query expansion for refining the search scope. Extensive experiments show that these optimizations greatly improve both memory recall and downstream question answering on LongMemEval. Overall, our study provides valuable resources and guidance for advancing the long-term memory capabilities of LLM-based chat assistants, paving the way toward more personalized and reliable conversational AI. Our benchmark and code are publicly available at https://github.com/xiaowu0162/LongMemEval.

## What This Adds (fill in)

- Problem: Long-term memory behavior in chat assistants is underexplored; we need targeted evaluation beyond "long context".
- Memory mechanism (write/update/retrieve/forget): Frames long-term memory as 3 stages: indexing, retrieval, reading; suggests optimizations (session decomposition, fact-augmented key expansion, time-aware query expansion).
- Evaluation / benchmarks: LongMemEval benchmark (500 questions) covering extraction, multi-session reasoning, temporal reasoning, knowledge updates, abstention.
- Failure modes / tradeoffs: Naive retrieval or naive retention leads to accuracy drops; time and update reasoning are common failure points.

## How This Maps To game-dev-memory (fill in)

- What to store (tables/assets): Store session boundaries explicitly and attach time-aware metadata to memories/assets; store "knowledge update" events (supersedes/contradicts).
- Retrieval pattern: Index per-session summaries and per-asset chunks; expand queries with extracted entities (engine version, map name, error code) and narrow by time range when appropriate.
- Evolution trigger: On session close, decompose and summarize by topic/value; periodically re-index as facts change (e.g., engine upgrades).
- UI affordance: Time-scope controls ("last 7 days", "since engine upgrade") and a "why retrieved" explanation that surfaces indexing keys/time filters.
