# MemGPT: Towards LLMs as Operating Systems

- arXiv: 2310.08560
- URL: https://arxiv.org/abs/2310.08560v2
- PDF: https://arxiv.org/pdf/2310.08560v2
- Published: 2023-10-12T17:51:32Z
- Updated: 2024-02-12T18:59:46Z
- Categories: cs.AI
- Authors: Charles Packer, Sarah Wooders, Kevin Lin, Vivian Fang, Shishir G. Patil, Ion Stoica, Joseph E. Gonzalez

## Abstract

Large language models (LLMs) have revolutionized AI, but are constrained by limited context windows, hindering their utility in tasks like extended conversations and document analysis. To enable using context beyond limited context windows, we propose virtual context management, a technique drawing inspiration from hierarchical memory systems in traditional operating systems that provide the appearance of large memory resources through data movement between fast and slow memory. Using this technique, we introduce MemGPT (Memory-GPT), a system that intelligently manages different memory tiers in order to effectively provide extended context within the LLM's limited context window, and utilizes interrupts to manage control flow between itself and the user. We evaluate our OS-inspired design in two domains where the limited context windows of modern LLMs severely handicaps their performance: document analysis, where MemGPT is able to analyze large documents that far exceed the underlying LLM's context window, and multi-session chat, where MemGPT can create conversational agents that remember, reflect, and evolve dynamically through long-term interactions with their users. We release MemGPT code and data for our experiments at https://memgpt.ai.

## What This Adds (fill in)

- Problem: LLM context windows cap long-running work (multi-session chat, long documents) unless memory is explicitly managed.
- Memory mechanism (write/update/retrieve/forget): Treat memory as tiers (fast "working" context vs slower long-term store) with explicit paging/summarization and control-flow "interrupts".
- Evaluation / benchmarks: Document analysis beyond context length; multi-session chat with evolving long-term interactions.
- Failure modes / tradeoffs: Added system complexity and latency; bad paging/summarization can lose critical details; needs strong evidence links to avoid reinforcing incorrect memories.

## How This Maps To game-dev-memory (fill in)

- What to store (tables/assets): Store raw evidence as assets (logs/traces/dumps) and keep "virtual context" summaries as memories linked to byte ranges/chunks.
- Retrieval pattern: Page in evidence chunks progressively (range fetch) + session summaries; prefer hybrid rank (FTS + recency + optional vectors) instead of giant prompts.
- Evolution trigger: Session close should generate a compact "session summary" and optionally a "procedure" memory if a reusable workflow was discovered.
- UI affordance: Expose explicit actions like "summarize this session", "pin this evidence chunk", "mark as superseded", "forget/delete with audit".
