# How Memory Management Impacts LLM Agents: An Empirical Study of Experience-Following Behavior

- arXiv: 2505.16067
- URL: https://arxiv.org/abs/2505.16067v2
- PDF: https://arxiv.org/pdf/2505.16067v2
- Published: 2025-05-21T22:35:01Z
- Updated: 2025-10-10T20:27:30Z
- Categories: cs.AI
- Authors: Zidi Xiong, Yuping Lin, Wenya Xie, Pengfei He, Zirui Liu, Jiliang Tang, Himabindu Lakkaraju, Zhen Xiang

## Abstract

Memory is a critical component in large language model (LLM)-based agents, enabling them to store and retrieve past executions to improve task performance over time. In this paper, we conduct an empirical study on how memory management choices impact the LLM agents' behavior, especially their long-term performance. Specifically, we focus on two fundamental memory management operations that are widely used by many agent frameworks-memory addition and deletion-to systematically study their impact on the agent behavior. Through our quantitative analysis, we find that LLM agents display an experience-following property: high similarity between a task input and the input in a retrieved memory record often results in highly similar agent outputs. Our analysis further reveals two significant challenges associated with this property: error propagation, where inaccuracies in past experiences compound and degrade future performance, and misaligned experience replay, where some seemingly correct executions can provide limited or even misleading value as experiences. Through controlled experiments, we demonstrate the importance of regulating experience quality within the memory bank and show that future task evaluations can serve as free quality labels for stored memory. Our findings offer insights into the behavioral dynamics of LLM agent memory systems and provide practical guidance for designing memory components that support robust, long-term agent performance.

## What This Adds (fill in)

- Problem: Memory addition/deletion choices change long-term agent behavior; we need to understand the dynamics.
- Memory mechanism (write/update/retrieve/forget): Studies memory addition + deletion; identifies "experience-following" (similar retrieved inputs drive similar outputs) and highlights error propagation + misaligned experience replay.
- Evaluation / benchmarks: Controlled experiments on agent tasks; proposes using future task outcomes as "free" quality labels for stored experiences.
- Failure modes / tradeoffs: Storing low-quality experiences can poison future behavior; deleting too aggressively loses useful patterns; requires quality signals and audit.

## How This Maps To game-dev-memory (fill in)

- What to store (tables/assets): Store outcome signals (success/failure, regression fixed?) alongside session memories and workflow traces; keep "quality labels" on procedural memories.
- Retrieval pattern: Weight retrieval by quality/confidence; avoid replaying low-quality experiences unless explicitly requested ("show failures like this").
- Evolution trigger: After an issue is resolved, backfill labels onto the evidence and procedure memories used; prune or supersede memories that repeatedly lead to bad actions.
- UI affordance: A visible "quality" indicator per memory/workflow and the ability to quarantine memories (do not retrieve by default).
