# Preference-Aware Memory Update for Long-Term LLM Agents

- arXiv: 2510.09720
- URL: https://arxiv.org/abs/2510.09720v1
- PDF: https://arxiv.org/pdf/2510.09720v1
- Published: 2025-10-10T06:49:35Z
- Updated: 2025-10-10T06:49:35Z
- Categories: cs.CL, cs.AI
- Authors: Haoran Sun, Zekun Zhang, Shaoning Zeng

## Abstract

One of the key factors influencing the reasoning capabilities of LLM-based agents is their ability to leverage long-term memory. Integrating long-term memory mechanisms allows agents to make informed decisions grounded in historical interactions. While recent advances have significantly improved the storage and retrieval components, by encoding memory into dense vectors for similarity search or organizing memory as structured knowledge graphs most existing approaches fall short in memory updating. In particular, they lack mechanisms for dynamically refining preference memory representations in response to evolving user behaviors and contexts. To address this gap, we propose a Preference-Aware Memory Update Mechanism (PAMU) that enables dynamic and personalized memory refinement. By integrating sliding window averages (SW) with exponential moving averages (EMA), PAMU constructs a fused preference-aware representation that captures both short-term fluctuations and long-term user tendencies. We conduct experiments on five task scenarios of the LoCoMo dataset, and the results show that our mechanism can significantly improve the output quality of LLM in five baselines, validating its effectiveness in long-term conversations.

## What This Adds (fill in)

- Problem:
- Memory mechanism (write/update/retrieve/forget):
- Evaluation / benchmarks:
- Failure modes / tradeoffs:

## How This Maps To game-dev-memory (fill in)

- What to store (tables/assets):
- Retrieval pattern:
- Evolution trigger:
- UI affordance: