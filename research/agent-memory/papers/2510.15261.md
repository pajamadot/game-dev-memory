# AUGUSTUS: An LLM-Driven Multimodal Agent System with Contextualized User Memory

- arXiv: 2510.15261
- URL: https://arxiv.org/abs/2510.15261v1
- PDF: https://arxiv.org/pdf/2510.15261v1
- Published: 2025-10-17T02:58:22Z
- Updated: 2025-10-17T02:58:22Z
- Categories: cs.AI
- Authors: Jitesh Jain, Shubham Maheshwari, Ning Yu, Wen-mei Hwu, Humphrey Shi

## Abstract

Riding on the success of LLMs with retrieval-augmented generation (RAG), there has been a growing interest in augmenting agent systems with external memory databases. However, the existing systems focus on storing text information in their memory, ignoring the importance of multimodal signals. Motivated by the multimodal nature of human memory, we present AUGUSTUS, a multimodal agent system aligned with the ideas of human memory in cognitive science. Technically, our system consists of 4 stages connected in a loop: (i) encode: understanding the inputs; (ii) store in memory: saving important information; (iii) retrieve: searching for relevant context from memory; and (iv) act: perform the task. Unlike existing systems that use vector databases, we propose conceptualizing information into semantic tags and associating the tags with their context to store them in a graph-structured multimodal contextual memory for efficient concept-driven retrieval. Our system outperforms the traditional multimodal RAG approach while being 3.5 times faster for ImageNet classification and outperforming MemGPT on the MSC benchmark.

## What This Adds (fill in)

- Problem:
- Memory mechanism (write/update/retrieve/forget):
- Evaluation / benchmarks:
- Failure modes / tradeoffs:

## How This Maps To game-dev-memory (fill in)

- What to store (tables/assets):
- Retrieval pattern:
- Evolution trigger:
- UI affordance: